{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name: \n"
     ]
    }
   ],
   "source": [
    "# Import xml parser\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fuzzy_pandas  as fpd\n",
    "\n",
    "# Enter the file name/well name\n",
    "print('Enter file name: ')\n",
    "file_name = input()\n",
    "print('file name: ' + file_name)\n",
    "print('Enter the most recent WBS')\n",
    "wbs = input()\n",
    "print('Recent WBS:' + wbs)\n",
    "\n",
    "tree = ET.parse(r\"C:\\Users\\\\Documents\"+str(file_name)+\".edm.xml\")\n",
    "\n",
    "root = tree.getroot()\n",
    "elemlist = []\n",
    "\n",
    "#CD_WELL (get the well name and API no)\n",
    "well_cols = [\"WELL_ID\",\"API_NO\",\"WELL_LEGAL_NAME\"]\n",
    "well_rows=[]\n",
    "for i in root.findall('CD_WELL'):\n",
    "        WELL_ID = i.get('WELL_ID')\n",
    "        API_NO = i.get('API_NO')\n",
    "        SPUD_DATE = i.get('SPUD_DATE')\n",
    "        WELL_LEGAL_NAME = i.get('WELL_LEGAL_NAME')   \n",
    "        well_rows.append({\"WELL_ID\": WELL_ID,\"API_NO\": API_NO,\"WELL_LEGAL_NAME\": WELL_LEGAL_NAME})\n",
    "wellname = pd.DataFrame(well_rows, columns = well_cols).drop_duplicates()\n",
    "# print(wellname)\n",
    "# DM_PIPE_DATA (Get the size of all pipes used in the well)\n",
    "pipe_cols = ['WELL_ID','EVENT_ID','PIPE_RUN_ID','PIPE_DATA_ID','SECT_TYPE_CODE','DESCRIPTION','NOMINAL_SIZE','THREAD_TYPE']\n",
    "pipe_rows = []\n",
    "for i in root.findall('DM_PIPE_DATA'):\n",
    "    WELL_ID = i.get('WELL_ID')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    PIPE_RUN_ID = i.get('PIPE_RUN_ID')\n",
    "    PIPE_DATA_ID = i.get('PIPE_DATA_ID')\n",
    "    SECT_TYPE_CODE = i.get('SECT_TYPE_CODE')\n",
    "    DESCRIPTION = i.get('DESCRIPTION')\n",
    "    NOMINAL_SIZE = i.get('NOMINAL_SIZE')\n",
    "    THREAD_TYPE = i.get('THREAD_TYPE')\n",
    "    pipe_rows.append({'WELL_ID':WELL_ID,'EVENT_ID':EVENT_ID,'PIPE_RUN_ID':PIPE_RUN_ID,'PIPE_DATA_ID':PIPE_DATA_ID,'SECT_TYPE_CODE':SECT_TYPE_CODE,'DESCRIPTION':DESCRIPTION,'NOMINAL_SIZE':NOMINAL_SIZE,'THREAD_TYPE':THREAD_TYPE})\n",
    "pipedata = pd.DataFrame(pipe_rows, columns = pipe_cols).drop_duplicates()\n",
    "# pipedata.head()\n",
    "#DM_REPORT_JOURNAL (get the list of all relevant reports)\n",
    "reports_cols = ['REPORT_JOURNAL_ID','EVENT_ID','DESCRIPTION','CREATE_DATE','ENTITY_TYPE','EVENT_CODE','REPORT_ALIAS']\n",
    "reports_rows = []\n",
    "events= ['DON','CON']\n",
    "desc =['PLUG','LINER','CBP','HANGER','FISH','RUN']\n",
    "alias = ['CASING','GEN_TRAN','GEN_PIPE','WB_EQUIP']\n",
    "desc_pattern = '|'.join(desc)\n",
    "for i in root.findall('DM_REPORT_JOURNAL'):\n",
    "    REPORT_JOURNAL_ID = i.get('REPORT_JOURNAL_ID')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    DESCRIPTION = i.get('DESCRIPTION')\n",
    "    CREATE_DATE = i.get('CREATE_DATE')\n",
    "    ENTITY_TYPE = i.get('ENTITY_TYPE')\n",
    "    EVENT_CODE = i.get('EVENT_CODE')\n",
    "    REPORT_ALIAS = i.get('REPORT_ALIAS')\n",
    "    reports_rows.append({'REPORT_JOURNAL_ID':REPORT_JOURNAL_ID,'EVENT_ID':EVENT_ID,'DESCRIPTION':DESCRIPTION,'CREATE_DATE':CREATE_DATE,'ENTITY_TYPE':ENTITY_TYPE,'EVENT_CODE':EVENT_CODE,'REPORT_ALIAS':REPORT_ALIAS})\n",
    "reports = pd.DataFrame(reports_rows, columns = reports_cols).drop_duplicates()\n",
    "reports = reports[reports.EVENT_CODE.isin(events)]\n",
    "reports = reports[reports.REPORT_ALIAS.isin(alias)]\n",
    "reports = reports.loc[~(reports['DESCRIPTION'].str.contains(desc_pattern, case=False))]\n",
    "# reports.head()\n",
    "well = wellname.WELL_LEGAL_NAME[0]\n",
    "#DM_PIPE_RUN (shows tally type, we need to get the offload)\n",
    "sumofflist_cols = [\"PIPE_RUN_ID\",\"TOTAL_LENGTH\",\"RUN_TALLY_TYPE\",\"RUN_LENGTH\",\"WELLBORE_ID\"]\n",
    "sumofflist_rows=[]\n",
    "ctlg = ['PUP','WELLHEAD','LANDING','MARKER','CROSS']\n",
    "ctlg_desc = '|'.join(ctlg)\n",
    "for i in root.findall('DM_PIPE_RUN'):\n",
    "    PIPE_RUN_ID = i.get('PIPE_RUN_ID')\n",
    "    TOTAL_LENGTH = i.get('TOTAL_LENGTH')\n",
    "    RUN_TALLY_TYPE = i.get('RUN_TALLY_TYPE')\n",
    "    RUN_LENGTH = i.get('RUN_LENGTH')\n",
    "    WELLBORE_ID = i.get('WELLBORE_ID')\n",
    "\n",
    "    sumofflist_rows.append({\"PIPE_RUN_ID\": PIPE_RUN_ID, \"TOTAL_LENGTH\": TOTAL_LENGTH,\"RUN_TALLY_TYPE\": RUN_TALLY_TYPE, \"RUN_LENGTH\": RUN_LENGTH, \"WELLBORE_ID\": WELLBORE_ID})\n",
    "    \n",
    "sumoff = pd.DataFrame(sumofflist_rows, columns = sumofflist_cols).drop_duplicates()\n",
    "# sumoff.head()\n",
    "#DM_PIPE_TALLY (get the sum of joints and length per pipe, need to filter RUN_TALLY_TYPE = OFF)\n",
    "#Verified \n",
    "#Tubing ofthen doesn't have offload or transfer report \n",
    "tally_cols = ['WELL_ID','EVENT_ID','PIPE_RUN_ID','PIPE_TALLY_ID','PIPE_DATA_ID','LENGTH','JOINT_NUMBER']\n",
    "tally_rows = []\n",
    "for i in root.findall('DM_PIPE_TALLY'):\n",
    "    WELL_ID = i.get('WELL_ID')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    PIPE_RUN_ID = i.get('PIPE_RUN_ID')\n",
    "    PIPE_TALLY_ID = i.get('PIPE_TALLY_ID')\n",
    "    PIPE_DATA_ID = i.get('PIPE_DATA_ID')\n",
    "    LENGTH = i.get('LENGTH')\n",
    "    JOINT_NUMBER = i.get('JOINT_NUMBER')\n",
    "    \n",
    "    tally_rows.append({'WELL_ID':WELL_ID,'EVENT_ID':EVENT_ID,'PIPE_RUN_ID':PIPE_RUN_ID,'PIPE_TALLY_ID':PIPE_TALLY_ID,'PIPE_DATA_ID':PIPE_DATA_ID,'LENGTH':LENGTH,'JOINT_NUMBER':JOINT_NUMBER})\n",
    "tally = pd.DataFrame(tally_rows, columns = tally_cols).drop_duplicates()\n",
    "tally['LENGTH'] = tally['LENGTH'].astype(float).round(2)\n",
    "tally = pd.merge(tally,wellname[[\"WELL_ID\",\"WELL_LEGAL_NAME\"]], on ='WELL_ID')\n",
    "tally = pd.merge(tally,pipedata[['PIPE_DATA_ID','SECT_TYPE_CODE','DESCRIPTION','NOMINAL_SIZE','THREAD_TYPE']], on ='PIPE_DATA_ID')\n",
    "tally = pd.merge(tally, sumoff[['PIPE_RUN_ID','RUN_TALLY_TYPE']], on = 'PIPE_RUN_ID')\n",
    "tally = tally[tally[\"RUN_TALLY_TYPE\"] == 'OFF']\n",
    "tally['THREAD_TYPE'].fillna(\"NA\",inplace = True)\n",
    "tally['NOMINAL_SIZE'].fillna(\"NA\",inplace = True)\n",
    "tally['NO_JOINTS'] = '1'\n",
    "tally = tally.loc[~(tally['DESCRIPTION'].str.contains(ctlg_desc,case=False))]\n",
    "tally['KEYWORD'] = tally['SECT_TYPE_CODE']+' '+tally['NOMINAL_SIZE'] + ' ' + tally['THREAD_TYPE']\n",
    "offload = tally.groupby(['WELL_LEGAL_NAME','NOMINAL_SIZE','DESCRIPTION']).agg({'LENGTH':'sum', 'NO_JOINTS':np.count_nonzero })\n",
    "# offload\n",
    "\n",
    "#DM_TRANSFER_SUMMARY (Get the summary transfer data)\n",
    "tfrsum_cols = ['WELL_ID','EVENT_ID','TRANSFER_ID','TRANSFER_SUMMARY_ID','ORIGIN','DESTINATION','REASON','AGENT']\n",
    "tfrsum_rows = []\n",
    "desc = ['PUP','WELLHEAD','LANDING','MARKER','CROSS']\n",
    "desc_remove = '|'.join(desc)\n",
    "origin = ['PREMIER','TUBOSCOPE','TENARIS']\n",
    "for i in root.findall('DM_TRANSFER_SUMMARY'):\n",
    "    WELL_ID = i.get('WELL_ID')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    TRANSFER_ID = i.get('TRANSFER_ID')\n",
    "    TRANSFER_SUMMARY_ID = i.get('TRANSFER_SUMMARY_ID')\n",
    "    ORIGIN = i.get('ORIGIN')\n",
    "    DESTINATION = i.get('DESTINATION')\n",
    "    REASON = i.get('REASON')\n",
    "    AGENT = i.get('AGENT')\n",
    "    \n",
    "    \n",
    "    tfrsum_rows.append({'WELL_ID': WELL_ID,'EVENT_ID': EVENT_ID,'TRANSFER_ID': TRANSFER_ID,\n",
    "                        'TRANSFER_SUMMARY_ID': TRANSFER_SUMMARY_ID,'ORIGIN': ORIGIN,\n",
    "                        'DESTINATION': DESTINATION,'REASON': REASON, 'AGENT': AGENT})\n",
    "tfrsum = pd.DataFrame(tfrsum_rows, columns = tfrsum_cols).drop_duplicates()\n",
    "\n",
    "# tfrsum\n",
    "#DM_TRANSFER_DETAIL (Get all Transfer Data)\n",
    "transfer_cols = ['WELL_ID','EVENT_ID','TRANSFER_ID','TRANSFER_SUMMARY_ID','TRANSFER_DETAIL_ID','DESCRIPTION','QUANTITY','UNITS','MANUFACTURER_SERIAL_NUMBER']\n",
    "transfer_rows = []\n",
    "for i in root.findall('DM_TRANSFER_DETAIL'):\n",
    "    WELL_ID = i.get('WELL_ID')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    TRANSFER_ID = i.get('TRANSFER_ID')\n",
    "    TRANSFER_SUMMARY_ID = i.get('TRANSFER_SUMMARY_ID')\n",
    "    TRANSFER_DETAIL_ID = i.get('TRANSFER_DETAIL_ID')\n",
    "    DESCRIPTION = i.get('DESCRIPTION')\n",
    "    QUANTITY = i.get('QUANTITY')\n",
    "    UNITS = i.get('UNITS')\n",
    "    MANUFACTURER_SERIAL_NUMBER = i.get('MANUFACTURER_SERIAL_NUMBER')\n",
    "    \n",
    "    transfer_rows.append({'WELL_ID': WELL_ID,'EVENT_ID':EVENT_ID,'TRANSFER_ID':TRANSFER_ID,'TRANSFER_SUMMARY_ID':TRANSFER_SUMMARY_ID,'TRANSFER_DETAIL_ID':TRANSFER_DETAIL_ID,'DESCRIPTION':DESCRIPTION,'QUANTITY':QUANTITY,'UNITS':UNITS,'MANUFACTURER_SERIAL_NUMBER': MANUFACTURER_SERIAL_NUMBER})\n",
    "transfer = pd.DataFrame(transfer_rows, columns = transfer_cols).drop_duplicates()\n",
    "transfer['QUANTITY'] = transfer['QUANTITY'].astype(float)\n",
    "transfer = pd.merge(transfer,wellname[[\"WELL_ID\",\"WELL_LEGAL_NAME\"]], on ='WELL_ID')\n",
    "transfer = pd.merge(transfer,tfrsum[['EVENT_ID','TRANSFER_ID','TRANSFER_SUMMARY_ID','ORIGIN','DESTINATION','REASON','AGENT']],on = ['EVENT_ID','TRANSFER_ID','TRANSFER_SUMMARY_ID'] )\n",
    "transfer['in/out'] = np.where(transfer['DESTINATION']==wellname.WELL_LEGAL_NAME[0], 'transfer in', 'transfer out')\n",
    "transfer['DESCRIPTION'] = transfer['DESCRIPTION'].str.replace(',','').str.replace('40#','').str.replace('BUTTRESS','').str.replace('23#','').str.replace('5-1/2\"','5.5').str.replace('9-5/8\"','9.625')\n",
    "transfer = transfer.loc[~(transfer['DESCRIPTION'].str.contains(desc_remove,case=False))]\n",
    "transfer = transfer[~(transfer.ORIGIN.isin(origin))]\n",
    "transferdata = transfer.groupby(['WELL_LEGAL_NAME','in/out','ORIGIN','DESTINATION','DESCRIPTION','REASON', 'MANUFACTURER_SERIAL_NUMBER']).agg({'QUANTITY': 'sum'})     \n",
    "transferdata = transferdata.reset_index()\n",
    "# transferdata\n",
    "\n",
    "# CD_ASSEMBLY\n",
    "asm_cols = ['ASSEMBLY_ID','REPORT_JOURNAL_ID','STRING_TYPE','DATE_REPORT','ASSEMBLY_NAME','ASSEMBLY_SIZE','LENGTH_TOTAL','EVENT_ID']\n",
    "asm_rows = []\n",
    "for i in root.findall('CD_ASSEMBLY'):\n",
    "    ASSEMBLY_ID = i.get('ASSEMBLY_ID')\n",
    "    REPORT_JOURNAL_ID = i.get('REPORT_JOURNAL_ID')\n",
    "    STRING_TYPE = i.get('STRING_TYPE')\n",
    "    DATE_REPORT = i.get('DATE_REPORT')\n",
    "    ASSEMBLY_NAME = i.get('ASSEMBLY_NAME')\n",
    "    ASSEMBLY_SIZE = i.get('ASSEMBLY_SIZE')\n",
    "    LENGTH_TOTAL = i.get('LENGTH_TOTAL')\n",
    "    EVENT_ID = i.get('EVENT_ID')\n",
    "    \n",
    "    asm_rows.append({'ASSEMBLY_ID':ASSEMBLY_ID,'REPORT_JOURNAL_ID':REPORT_JOURNAL_ID,'STRING_TYPE':STRING_TYPE,'DATE_REPORT':DATE_REPORT,'ASSEMBLY_NAME':ASSEMBLY_NAME,'ASSEMBLY_SIZE':ASSEMBLY_SIZE,'LENGTH_TOTAL':LENGTH_TOTAL,'EVENT_ID':EVENT_ID})\n",
    "sect1 = ['CAS','TBG']\n",
    "asm_name = ['CONDUCTOR','PACKER']\n",
    "ctlg = ['PUP','WELLHEAD','LANDING','MARKER','CROSS']\n",
    "asm_name_desc = '|'.join(asm_name)\n",
    "ctlg_desc = '|'.join(ctlg)\n",
    "asm = pd.DataFrame(asm_rows, columns = asm_cols).drop_duplicates()\n",
    "asm = pd.merge(asm,reports[['REPORT_JOURNAL_ID','EVENT_ID','DESCRIPTION','ENTITY_TYPE']], on = ['REPORT_JOURNAL_ID','EVENT_ID'])\n",
    "# asm\n",
    "#CD_ASSEMBLY_COMP(Burried report for Casing and Tubing) \n",
    "asmbly_cols = ['WELL_ID','ASSEMBLY_ID','ASSEMBLY_COMP_ID','SECT_TYPE_CODE','CATALOG_KEY_DESC','COMP_NAME',\n",
    "              'DESCRIPTION','LENGTH','OD_BODY','SEQUENCE_NO','COMP_TYPE_CODE','CREATE_USER_ID',\n",
    "               'CREATE_DATE','GRADE','JOINTS','CONNECTION_NAME','NOMINAL_SIZE']\n",
    "asmbly_rows = []\n",
    "for i in root.findall('CD_ASSEMBLY_COMP'):\n",
    "    WELL_ID = i.get('WELL_ID')\n",
    "    ASSEMBLY_ID = i.get('ASSEMBLY_ID')\n",
    "    ASSEMBLY_COMP_ID = i.get('ASSEMBLY_COMP_ID')\n",
    "    SECT_TYPE_CODE = i.get('SECT_TYPE_CODE')\n",
    "    CATALOG_KEY_DESC = i.get('CATALOG_KEY_DESC')\n",
    "    COMP_NAME = i.get('COMP_NAME')\n",
    "    DESCRIPTION = i.get('DESCRIPTION')\n",
    "    LENGTH = i.get('LENGTH')\n",
    "    ID_BODY = i.get('ID_BODY')\n",
    "    MD_BASE = i.get('MD_BASE')\n",
    "    MD_TOP = i.get('MD_TOP')\n",
    "    OD_BODY = i.get('OD_BODY')\n",
    "    SEQUENCE_NO = i.get('SEQUENCE_NO')\n",
    "    COMP_TYPE_CODE = i.get('COMP_TYPE_CODE')\n",
    "    CREATE_USER_ID = i.get('CREATE_USER_ID')\n",
    "    CREATE_DATE = i.get('CREATE_DATE')\n",
    "    GRADE = i.get('GRADE')\n",
    "    JOINTS = i.get('JOINTS')\n",
    "    CONNECTION_NAME = i.get('CONNECTION_NAME')\n",
    "    NOMINAL_SIZE = i.get('NOMINAL_SIZE')\n",
    "    \n",
    "    asmbly_rows.append({'WELL_ID':WELL_ID,'ASSEMBLY_ID':ASSEMBLY_ID,'ASSEMBLY_COMP_ID':ASSEMBLY_COMP_ID,'SECT_TYPE_CODE':SECT_TYPE_CODE,'CATALOG_KEY_DESC':CATALOG_KEY_DESC,'COMP_NAME':COMP_NAME,\n",
    "              'DESCRIPTION':DESCRIPTION,'LENGTH':LENGTH,'OD_BODY':OD_BODY,'SEQUENCE_NO':SEQUENCE_NO,'COMP_TYPE_CODE':COMP_TYPE_CODE,'CREATE_USER_ID':CREATE_USER_ID,\n",
    "               'CREATE_DATE':CREATE_DATE.strip('{').strip('}').strip('ts').strip(\"'\").strip(\" '\"),'GRADE':GRADE,'JOINTS':JOINTS,'CONNECTION_NAME':CONNECTION_NAME,'NOMINAL_SIZE':NOMINAL_SIZE})\n",
    "asmbly = pd.DataFrame(asmbly_rows, columns = asmbly_cols).drop_duplicates()\n",
    "asmbly['LENGTH'] = asmbly['LENGTH'].astype(float).round(2)\n",
    "asmbly['JOINTS'] = asmbly['JOINTS'].astype(float)\n",
    "asmbly = pd.merge(asmbly,wellname[['WELL_ID','WELL_LEGAL_NAME']], on = 'WELL_ID')\n",
    "asmbly = pd.merge(asmbly,asm[['ASSEMBLY_ID','ASSEMBLY_NAME','DESCRIPTION','ENTITY_TYPE']], on = 'ASSEMBLY_ID')\n",
    "asmbly = asmbly[asmbly['SECT_TYPE_CODE'].isin(sect1)]\n",
    "asmbly = asmbly.loc[~(asmbly['ASSEMBLY_NAME'].str.contains(asm_name_desc, case=False))]\n",
    "asmbly = asmbly.loc[~(asmbly['CATALOG_KEY_DESC'].str.contains(ctlg_desc,case=False))]\n",
    "burried = asmbly.groupby(['WELL_LEGAL_NAME','OD_BODY','CATALOG_KEY_DESC']).agg({'LENGTH':'sum', 'JOINTS':'sum'})\n",
    "# burried\n",
    "\n",
    "#Create an export table \n",
    "column_names = ['Well Name','WBS-Element','OD','Material description (Dia, wt#, Grade, Conn)',\n",
    "                'Delivered from Supplier(jts)', 'Delivered from Supplier(ft)', 'Transfer In(jts)', \n",
    "                'Transfer In(ft)','Off Load Report(jts)','Off Load Report(ft)','Burried(jts)','Burried(ft)',\n",
    "                'Burried Thread Loss Footage(Ft)','Transfer Out(jts)','Transfer Out(ft)','Difference(jts)', 'Diference(ft)', 'Comments']\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "df_burried = burried.reset_index()\n",
    "df_burried.rename(columns = {df_burried.columns[2]:'DESCRIPTION'}, inplace = True)\n",
    "df1 = df_burried.merge(offload.reset_index(),on=['DESCRIPTION'], how='left')\n",
    "df1['KEYWORD'] = df1.DESCRIPTION + ' ' + df1.OD_BODY\n",
    "df1['KEYWORD'] = df1['KEYWORD'].str.replace(\"JOINTS\",\"\")\n",
    "transfer_in = transferdata[transferdata['in/out'] == 'transfer in']\n",
    "transfer_out = transferdata[transferdata['in/out'] == 'transfer out']\n",
    "dftransfer = pd.merge(transfer_in, transfer_out, on = ['DESCRIPTION'], how = 'left')\n",
    "# dftransfer\n",
    "df1.pop('DESCRIPTION')\n",
    "df2 = fpd.fuzzy_merge(df1, dftransfer, left_on = ['KEYWORD'], join = 'left-outer',right_on=['DESCRIPTION'],method = 'levenshtein', ignore_case = True,ignore_order_words =True,threshold =0.60)\n",
    "\n",
    "# df2 = fpd.fuzzy_merge(df1, transfer_out, left_on = ['KEYWORD'], join = 'left-outer',right_on=['DESCRIPTION'],method = 'levenshtein', ignore_case = True,ignore_order_words =True,threshold =0.6)\n",
    "# df3 = fpd.fuzzy_merge(df2, transfer_in, left_on = ['KEYWORD'], join = 'left-outer',right_on=['DESCRIPTION'],method = 'levenshtein', ignore_case = True,ignore_order_words =True,threshold =0.6)\n",
    "        \n",
    "for i,r in df2.iterrows():\n",
    "    df = df.append({'Well Name': well\n",
    "                    , 'OD': df2['OD_BODY'].loc[i]\n",
    "                    ,'WBS-Element': wbs\n",
    "                    , 'Burried(jts)': df2['JOINTS'].loc[i]\n",
    "                    ,'Burried(ft)': df2['LENGTH_x'].loc[i]\n",
    "                    ,'Material description (Dia, wt#, Grade, Conn)': df2['KEYWORD'].loc[i]\n",
    "                    ,'Off Load Report(jts)': df2['NO_JOINTS'].loc[i]\n",
    "                    , 'Off Load Report(ft)': df2['LENGTH_y'].loc[i]\n",
    "                    ,'Transfer In(jts)': df2['QUANTITY_x'].loc[i]\n",
    "                    ,'Transfer In(ft)': df2['MANUFACTURER_SERIAL_NUMBER_x'].loc[i]\n",
    "                    ,'Transfer Out(jts)': df2['QUANTITY_y'].loc[i]\n",
    "                    ,'Transfer Out(ft)': df2['MANUFACTURER_SERIAL_NUMBER_y'].loc[i]\n",
    "#                     ,'Difference(jts)': '=' + 'D' + str(i) + '+' + 'F' + str(i) + '-' + 'J' + str(i) + '-' + 'M' + str(i)\n",
    "                    ,'Burried Thread Loss Footage(Ft)': df2['LENGTH_x'].loc[i]*0.01,'Comments': df2['REASON_y'].loc[i]}\n",
    "                   , ignore_index = True)\n",
    "\n",
    "df = df.fillna('').sort_values(by=['OD'],ascending = False ).reset_index(drop=True)\n",
    "df.pop('OD')\n",
    "\n",
    "#enter the excel formula after the table is sorted\n",
    "for i,r in df.iterrows():\n",
    "    df['Difference(jts)'].iloc[i] = '=' + 'D' + str(i+2) + '+' + 'F' + str(i+2) + '-' + 'J' + str(i+2) + '-' + 'M' + str(i+2)\n",
    "for i,r in df.iterrows():\n",
    "    df['Diference(ft)'].iloc[i] = '=(' + 'E' + str(i+2) + '+' + 'G' + str(i+2) + ')-(' + 'K' + str(i+2) + '+' + 'L' + str(i+2) +')-' + 'N' + str(i+2)\n",
    "#  =(E5+G5)-(K5+L5)-N5   \n",
    "\n",
    "#Name of agents performing transfer\n",
    "agent_transfer = transfer[[\"WELL_LEGAL_NAME\",'in/out','AGENT',\"DESCRIPTION\",\"ORIGIN\",'DESTINATION','REASON','QUANTITY','MANUFACTURER_SERIAL_NUMBER']]\n",
    "agent_transfer.rename(columns = {agent_transfer.columns[8]: 'LENGTH'}, inplace = True)\n",
    "#casing report creator\n",
    "casing_detail = asmbly[[\"ENTITY_TYPE\",\"CATALOG_KEY_DESC\",\"ASSEMBLY_NAME\",\"CREATE_USER_ID\",\"GRADE\",\"JOINTS\",\"CONNECTION_NAME\"]]\n",
    "\n",
    "#export the data into excel format to be reviewed\n",
    "\n",
    "writer = pd.ExcelWriter(r\"C:\\Users\\\\Documents\\Converted Files\"+file_name+\".xlsx\",engine='xlsxwriter')\n",
    "df.to_excel(writer,sheet_name=\"Report\",index=False)\n",
    "agent_transfer.to_excel(writer, sheet_name = \"detail transfer\", index = False)\n",
    "casing_detail.to_excel(writer,sheet_name = \"detail burried joints\", index = False)\n",
    "writer.save()\n",
    "\n",
    "\n",
    "#GL for Casing & Tubing 760215: Casing & TBG 760220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORT_JOURNAL_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>ENTITY_TYPE</th>\n",
       "      <th>EVENT_CODE</th>\n",
       "      <th>REPORT_ALIAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>KlDMmXABGJ</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>MT RECEIVED 5.5'' CASING</td>\n",
       "      <td>{ts '2019-11-12 15:11:57'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CIiSnNH6J2</td>\n",
       "      <td>eZqTW</td>\n",
       "      <td>5.5 PKR AND 2.875 PROD TBG</td>\n",
       "      <td>{ts '2020-02-13 10:50:04'}</td>\n",
       "      <td>Wellbore Equipment</td>\n",
       "      <td>CON</td>\n",
       "      <td>WB_EQUIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>HXaDRtZOhS</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>5.5'' OFF-LOAD TALLY</td>\n",
       "      <td>{ts '2019-11-16 10:43:40'}</td>\n",
       "      <td>Pipe Tally</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_PIPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9NaW657evo</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>5.5 PROD CASING REPORT</td>\n",
       "      <td>{ts '2019-11-21 16:26:05'}</td>\n",
       "      <td>Casing</td>\n",
       "      <td>DON</td>\n",
       "      <td>CASING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>USo0aj2aYG</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>TRANSFER 5.5\" CASING TO 63H</td>\n",
       "      <td>{ts '2019-11-21 16:56:57'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>QlNgPCVy8L</td>\n",
       "      <td>eZqTW</td>\n",
       "      <td>2.875 PROD TBG DELIVERED</td>\n",
       "      <td>{ts '2020-02-12 12:16:08'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>CON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>VZZHoR8wM2</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>9.625'' OFF LOAD TALLY</td>\n",
       "      <td>{ts '2019-10-16 21:20:08'}</td>\n",
       "      <td>Pipe Tally</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_PIPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>HXmmrvbshy</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>RECIEVE FROM JC MARTIN 63H</td>\n",
       "      <td>{ts '2019-10-21 07:31:53'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>BP9SXmA6uP</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>MATERIAL TRANSFER 9.625\" 27JTS TO JC MARTIN 65H</td>\n",
       "      <td>{ts '2019-10-19 12:03:05'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>B2xd2UkSN6</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>9.625\" SURFACE CASING REPORT</td>\n",
       "      <td>{ts '2019-10-19 17:22:21'}</td>\n",
       "      <td>Casing</td>\n",
       "      <td>DON</td>\n",
       "      <td>CASING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>KigSCCcE72</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>RECIEVE FROM PREMIER 9.625\"</td>\n",
       "      <td>{ts '2019-10-21 07:35:19'}</td>\n",
       "      <td>Material Transfer</td>\n",
       "      <td>DON</td>\n",
       "      <td>GEN_TRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>EqfeEf6HJa</td>\n",
       "      <td>xwrct</td>\n",
       "      <td>CONDUCTOR CASING</td>\n",
       "      <td>{ts '2019-10-06 06:15:14'}</td>\n",
       "      <td>Casing</td>\n",
       "      <td>DON</td>\n",
       "      <td>CASING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    REPORT_JOURNAL_ID EVENT_ID  \\\n",
       "20         KlDMmXABGJ    xwrct   \n",
       "25         CIiSnNH6J2    eZqTW   \n",
       "63         HXaDRtZOhS    xwrct   \n",
       "67         9NaW657evo    xwrct   \n",
       "75         USo0aj2aYG    xwrct   \n",
       "122        QlNgPCVy8L    eZqTW   \n",
       "125        VZZHoR8wM2    xwrct   \n",
       "128        HXmmrvbshy    xwrct   \n",
       "134        BP9SXmA6uP    xwrct   \n",
       "135        B2xd2UkSN6    xwrct   \n",
       "136        KigSCCcE72    xwrct   \n",
       "140        EqfeEf6HJa    xwrct   \n",
       "\n",
       "                                         DESCRIPTION  \\\n",
       "20                          MT RECEIVED 5.5'' CASING   \n",
       "25                        5.5 PKR AND 2.875 PROD TBG   \n",
       "63                              5.5'' OFF-LOAD TALLY   \n",
       "67                            5.5 PROD CASING REPORT   \n",
       "75                       TRANSFER 5.5\" CASING TO 63H   \n",
       "122                         2.875 PROD TBG DELIVERED   \n",
       "125                           9.625'' OFF LOAD TALLY   \n",
       "128                       RECIEVE FROM JC MARTIN 63H   \n",
       "134  MATERIAL TRANSFER 9.625\" 27JTS TO JC MARTIN 65H   \n",
       "135                     9.625\" SURFACE CASING REPORT   \n",
       "136                      RECIEVE FROM PREMIER 9.625\"   \n",
       "140                                 CONDUCTOR CASING   \n",
       "\n",
       "                    CREATE_DATE         ENTITY_TYPE EVENT_CODE REPORT_ALIAS  \n",
       "20   {ts '2019-11-12 15:11:57'}   Material Transfer        DON     GEN_TRAN  \n",
       "25   {ts '2020-02-13 10:50:04'}  Wellbore Equipment        CON     WB_EQUIP  \n",
       "63   {ts '2019-11-16 10:43:40'}          Pipe Tally        DON     GEN_PIPE  \n",
       "67   {ts '2019-11-21 16:26:05'}              Casing        DON       CASING  \n",
       "75   {ts '2019-11-21 16:56:57'}   Material Transfer        DON     GEN_TRAN  \n",
       "122  {ts '2020-02-12 12:16:08'}   Material Transfer        CON     GEN_TRAN  \n",
       "125  {ts '2019-10-16 21:20:08'}          Pipe Tally        DON     GEN_PIPE  \n",
       "128  {ts '2019-10-21 07:31:53'}   Material Transfer        DON     GEN_TRAN  \n",
       "134  {ts '2019-10-19 12:03:05'}   Material Transfer        DON     GEN_TRAN  \n",
       "135  {ts '2019-10-19 17:22:21'}              Casing        DON       CASING  \n",
       "136  {ts '2019-10-21 07:35:19'}   Material Transfer        DON     GEN_TRAN  \n",
       "140  {ts '2019-10-06 06:15:14'}              Casing        DON       CASING  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
